{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496d74b3",
   "metadata": {},
   "source": [
    "# Pytorch Basic Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf360be",
   "metadata": {},
   "source": [
    "In this notebook, we will cover the very basics of using pytorch. In all cases, the [pytorch documentation](https://pytorch.org/docs/stable/index.html) will be your best source for reference. We will be using pytorch for HW2, HW3 and it will be very helpful for your project. Pytorch is one of the most popular machine learning libraries which is used both in academia and industry.\n",
    "\n",
    "The resources this notebook heavily relies on are:\n",
    "1. CS224N Stanford Pytorch [tutorial](https://web.stanford.edu/class/cs224n/materials/CS224N_PyTorch_Tutorial.html).\n",
    "2. Udacity skip-gram [tutorial](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/Skip_Grams_Solution.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2f97dd",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "Tensors are the basic building blocks in Pytorch. They are similar to n-dimensional arrays you have encoutered in numpy.\n",
    "\n",
    "We'll first look at few different ways of creating tensors and some basic properties of them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23894257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Create tensors using list\n",
    "x1 = [[0, 1], [2, 3], [4, 5]]\n",
    "tensor1 = torch.tensor(x1)\n",
    "\n",
    "# Create tensors using numpy array\n",
    "x2 = np.array(x1)\n",
    "tensor2 = torch.from_numpy(x2)\n",
    "\n",
    "# Check data type and shape of tensor and device of tensors\n",
    "print(tensor1.dtype, tensor1.shape, tensor1.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43149d47",
   "metadata": {},
   "source": [
    "The indexing of tensors and operations on them are very similar to those used in numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686536cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a tensor of ones of size 5*2\n",
    "a = torch.ones(5,2)\n",
    "\n",
    "# Index a row of the tensor\n",
    "print(a[0,:])\n",
    "\n",
    "# Index a particular element\n",
    "print(a[3,1])\n",
    "\n",
    "# Get a python scalar corresponding to the item\n",
    "print(a[3,1].item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca48fc",
   "metadata": {},
   "source": [
    "The [documentation](https://pytorch.org/docs/stable/tensors.html) covers a lot of operations, but some of the useful ones are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f3da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a tensor with elemnts ranging from 0 to 9\n",
    "a = torch.arange(10)\n",
    "\n",
    "# Reshape tensor to (1,10)\n",
    "a = a.view(1,10)\n",
    "\n",
    "# Concatenate in dimension 0 and 1\n",
    "a_cat0 = torch.cat([a, a, a], dim=0)\n",
    "a_cat1 = torch.cat([a, a, a], dim=1)\n",
    "\n",
    "# Check shape of created tensors\n",
    "print(a_cat0.shape)\n",
    "print(a_cat1.shape)\n",
    "\n",
    "# Squeeze removes of dimension of size 1\n",
    "a = a.squeeze(dim=0)\n",
    "print(a.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c772b1",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "One of the main useful features for Pytorch is the automatic differentiation feature provided through Autograd i.e. once you define a computation, the backpropogation algorithm to compute the gradient of all weights is done automatically by Pytorch. The main useful method we will look at is `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4820396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example tensor\n",
    "# requires_grad parameter tells PyTorch to store gradients\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "# Print the gradient if it is calculated\n",
    "# Currently None since x is a scalar\n",
    "print(x.grad)\n",
    "\n",
    "y = x * x * 3 # 3x^2\n",
    "y.backward()\n",
    "print(x.grad) # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f2b64",
   "metadata": {},
   "source": [
    "If we run backpropogation again on a different operation for the same tensor, Pytorch accumulates the gradient i.e. calculates the sum of the gradients so far. One method to zero out the gradients (which is usually done in every training iteration) is by using `zero_grad()` function. We will look at this later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a new operation using the same tensor\n",
    "z = x * x * 3 # 3x^2\n",
    "z.backward()\n",
    "\n",
    "# Output will be sum of gradients so far\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9cd74f",
   "metadata": {},
   "source": [
    "## Neural Network Module\n",
    "\n",
    "Pytorch provides a lot of useful simple building blocks in `torch.nn` which can be useful to create complicated neural networks.\n",
    "\n",
    "### Linear layer\n",
    "\n",
    "One can create linary layers using the `nn.Linear(d_in, d_out)` where `d_in` is the dimension of the input tensor and `d_out` is the size of the output tensor. This will take a matrix of `(N, *, H_in)` dimensions and output a matrix of `(N, *, H_out)`. The `*` denotes that there could be arbitrary number of dimensions in between. The linear layer performs the operation `Ax+b`, where `A` and `b` are initialized randomly. If we don't want the linear layer to learn the bias parameters, we can initialize our layer with `bias=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f337257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create the inputs\n",
    "input = torch.ones(2,3,4)\n",
    "\n",
    "# Make a linear layers transforming N,*,H_in dimensinal inputs to N,*,H_out\n",
    "# dimensional outputs\n",
    "# The weights are randomly initialized\n",
    "linear = nn.Linear(4, 2)\n",
    "linear_output = linear(input)\n",
    "\n",
    "# Check shape of output\n",
    "print(linear_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b563dae",
   "metadata": {},
   "source": [
    "\n",
    "You can find many other useful module layers besides the linear layer such as recurrent layers (e.g. `nn.RNN` or `nn.LSTM`), pooling layers (e.g. `nn.MaxPool2d`), normalization layers (e.g. `nn.LayerNorm`), dropout layers (e.g. `nn.Dropout`), loss layers (e.g. `nn.BCELoss`) etc.\n",
    "\n",
    "### Activation Layers\n",
    "\n",
    "We can also use the nn module to apply activations functions to our tensors. Activation functions are used to add non-linearity to our network. Some examples of activations functions are `nn.ReLU()`, `nn.Sigmoid()` and `nn.Softmax()`. Activation functions operate on each element seperately, so the shape of the tensors we get as an output are the same as the ones we pass in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8575b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output of linear layer, which is input to activation layer\n",
    "print(linear_output)\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(linear_output)\n",
    "\n",
    "# Check output of activation layer\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fdbfc2",
   "metadata": {},
   "source": [
    "So far we have seen that we can create layers and pass the output of one as the input of the next. Instead of creating intermediate tensors and passing them around, we can use nn.Sequentual, which does exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = nn.Sequential(\n",
    "    nn.Linear(4, 2),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "input = torch.ones(2,3,4)\n",
    "output = block(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d263c",
   "metadata": {},
   "source": [
    "### Custom Modules\n",
    "\n",
    "Instead of using the predefined modules, we can also build our own by extending the `nn.Module` class. We can initialize our parameters in the `__init__` function, starting with a call to the `__init__` function of the super class. All the class attributes we define which are nn module objects are treated as parameters, which can be learned during the training. Tensors are not parameters, but they can be turned into parameters if they are wrapped in `nn.Parameter` class.\n",
    "\n",
    "All classes extending `nn.Module` are also expected to implement a `forward(x)` function, where `x` is a tensor. This is the function that is called when a parameter is passed to our module, such as in `model(x)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25fe06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, hidden_size):\n",
    "    # Call to the __init__ function of the super class\n",
    "    super(MultilayerPerceptron, self).__init__()\n",
    "\n",
    "    # Bookkeeping: Saving the initialization parameters\n",
    "    self.input_size = input_size \n",
    "    self.hidden_size = hidden_size \n",
    "\n",
    "    # Defining of our model\n",
    "    self.model = nn.Sequential(\n",
    "        nn.Linear(self.input_size, self.hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.hidden_size, self.input_size),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "  def forward(self, x):\n",
    "    output = self.model(x)\n",
    "    return output\n",
    "\n",
    "\n",
    "# Make a sample input\n",
    "input = torch.randn(2, 5)\n",
    "\n",
    "# Create our model\n",
    "model = MultilayerPerceptron(5, 3)\n",
    "\n",
    "# Pass our input through our model\n",
    "output = model(input)\n",
    "\n",
    "# Check output\n",
    "print(\"Output \", output, \"\\n\")\n",
    "\n",
    "# Check all parameters in our custom defined model\n",
    "print(\"Parameters\", list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f487b60c",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "\n",
    "We have seen how gradients are calculated with the `backward()` function. Having the gradients isn't enough for our models to learn. We also need to know how to update the parameters of our models. This is where the optimizers comes in. `torch.optim` module contains several optimizers that we can use. Some popular examples are `optim.SGD` and `optim.Adam`. When initializing optimizers, we pass our model parameters, which can be accessed with `model.parameters()`, telling the optimizers which values it will be optimizing. Optimizers also has a learning rate (`lr`) parameter, which determines how big of an update will be made in every step. Different optimizers have different hyperparameters as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Create the y data\n",
    "y = torch.ones(10, 5)\n",
    "\n",
    "# Add some noise to our goal y to generate our x\n",
    "# We want out model to predict our original data, albeit the noise\n",
    "x = y + torch.randn_like(y)\n",
    "\n",
    "# Instantiate the model\n",
    "model = MultilayerPerceptron(5, 3)\n",
    "\n",
    "# Define the optimizer\n",
    "adam = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Define loss using a predefined loss function\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# Calculate how our model is doing now\n",
    "y_pred = model(x)\n",
    "loss_function(y_pred, y).item()\n",
    "\n",
    "# Set the number of epoch, which determines the number of training iterations\n",
    "n_epoch = 10 \n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "  # Set the gradients to 0\n",
    "  adam.zero_grad()\n",
    "\n",
    "  # Get the model predictions\n",
    "  y_pred = model(x)\n",
    "\n",
    "  # Get the loss\n",
    "  loss = loss_function(y_pred, y)\n",
    "\n",
    "  # Print stats\n",
    "  print(f\"Epoch {epoch}: traing loss: {loss}\")\n",
    "\n",
    "  # Compute the gradients\n",
    "  loss.backward()\n",
    "\n",
    "  # Take a step to optimize the weights\n",
    "  adam.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217bd32",
   "metadata": {},
   "source": [
    "Let's check how our model performs on a data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c842e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how our model performs on the training data\n",
    "y_pred = model(x)\n",
    "print(y_pred)\n",
    "\n",
    "# Create test data and check how our model performs on it\n",
    "x2 = y + torch.randn_like(y)\n",
    "y_pred = model(x2)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a3f4c",
   "metadata": {},
   "source": [
    "## Real Example: Skip-gram Model\n",
    "\n",
    "In the last lecture, you covered word2vec skip-gram model where the task was to predict the context words given a particular word. We used pretrained vectors in the last notebook, but we will now look at a simple example of how to train these word vectors in Pytorch.\n",
    "\n",
    "We will be using the corpus [here](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip) (based on wikipedia) to train our word vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the extracted text file      \n",
    "with open('text8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# print out the first 100 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b0e795",
   "metadata": {},
   "source": [
    "Next, we will process the dataset using the tokenization functions from `nltk`. The dataset contains 16M tokens, but for faster training we are going to use 100k. You can comment out that part to train a model on all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a0fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove all words with  5 or fewer occurences\n",
    "    word_counts = Counter(words)\n",
    "    trimmed_words = [word for word in words if word_counts[word] > 5]\n",
    "\n",
    "    return trimmed_words\n",
    "\n",
    "# get list of words\n",
    "words = preprocess(text)\n",
    "print(words[:30])\n",
    "\n",
    "# for the purpose of illustration, we are going to restrict the size of dataset\n",
    "words = words[:100000]\n",
    "\n",
    "# print some stats about this word data\n",
    "print(\"Total words in text: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5428cbaa",
   "metadata": {},
   "source": [
    "Next, we are going to create dictionaries which map each word in our vocabulary to an integer and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652263a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: Two dictionaries, vocab_to_int, int_to_vocab\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    # sorting the words from most to least frequent in text occurrence\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    # create int_to_vocab dictionaries\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "train_words = [word for word in int_words]\n",
    "\n",
    "print(int_words[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e3e4ed",
   "metadata": {},
   "source": [
    "The original implementation of word2vec used some additional tricks to preprocess the data which we are going to exclude for simplicity here (e.g. to remove noise, they randomly drop some words).\n",
    "\n",
    "We'll next implement the fuction which gives us neighbouring words within a window w.r.t to a particular word. Instead of a fixed window size, we'll follow the original word2vec paper and select `R` randomly between `1` and `window_size` as the widow size for one call to the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4af3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = words[start:idx] + words[idx+1:stop+1]\n",
    "    \n",
    "    return list(target_words)\n",
    "\n",
    "int_text = [i for i in range(10)]\n",
    "print('Input: ', int_text)\n",
    "idx=5 # word index of interest\n",
    "\n",
    "target = get_target(int_text, idx=idx, window_size=5)\n",
    "print('Target: ', target)  # you should get some indices around the idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35982d4f",
   "metadata": {},
   "source": [
    "\n",
    "Next, let's implement a function which gives us batches of data which we can use to train our model. The generator function returns batches of input and target data for our model, using the get_target function from above. The idea is that it grabs batch_size words from a words list. Then for each of those batches, it gets the target words in a window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8518f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y\n",
    "        \n",
    "int_text = [i for i in range(20)]\n",
    "x,y = next(get_batches(int_text, batch_size=4, window_size=5))\n",
    "\n",
    "print('x\\n', x)\n",
    "print('y\\n', y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f59bc1",
   "metadata": {},
   "source": [
    "\n",
    "We'll define the skip-gram model using the functions you just learnt from `nn.Module` in Pytorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa7e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.output = nn.Linear(n_embed, n_vocab)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        scores = self.output(x)\n",
    "        log_ps = self.log_softmax(scores)\n",
    "        \n",
    "        return log_ps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d03bb3",
   "metadata": {},
   "source": [
    "Before moving on to the final training loop part of this code, we are going to define a function which we will use to validate our training. This is the same cosine similarity function you looked at last time, except we are going to use lookup in `nn.Embedding()` which contains all our word vectors, and find similarity for a random word in our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc37a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
    "    \n",
    "    # Here we're calculating the cosine similarity between some random words and \n",
    "    # our embedding vectors. With the similarities, we can look at what words are\n",
    "    # close to our random words.\n",
    "    \n",
    "    # sim = (a . b) / |a||b|\n",
    "    \n",
    "    embed_vectors = embedding.weight\n",
    "    \n",
    "    # magnitude of embedding vectors, |b|\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    \n",
    "    # pick N words from our ranges (0,window) and (1000,1000+window). lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples,\n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
    "        \n",
    "    return valid_examples, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf40fa",
   "metadata": {},
   "source": [
    "Finally we'll define the main training loop where we will iterate over batches of data, compute a forward pass, compute the gradients using the `backward()` function, and use the optimizer to update the model parameters.\n",
    "\n",
    "**NOTE**: The below training loop will be very slow to run on CPUs on your laptop, so I would not recommend it. You can instead run it on GPUs --- you should get access to the computing server very soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "embedding_dim=50 # you can change, if you want\n",
    "\n",
    "model = SkipGram(len(vocab_to_int), embedding_dim).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "eval_every = 500\n",
    "print_every = 10\n",
    "steps = 0\n",
    "epochs = 5\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # get input and target batches\n",
    "    for inputs, targets in get_batches(train_words, 512):\n",
    "        steps += 1\n",
    "        inputs, targets = torch.LongTensor(inputs), torch.LongTensor(targets)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        log_ps = model(inputs)\n",
    "        loss = criterion(log_ps, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            print(\"Loss: \", loss)\n",
    "\n",
    "        if steps % eval_every == 0:                  \n",
    "            # getting examples and similarities      \n",
    "            valid_examples, valid_similarities = cosine_similarity(model.embed, device=device)\n",
    "            _, closest_idxs = valid_similarities.topk(6) # topk highest similarities\n",
    "            \n",
    "            valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
    "            for ii, valid_idx in enumerate(valid_examples):\n",
    "                closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
    "                print(int_to_vocab[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n",
    "            print(\"...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52121e65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "08eb4fb58a4b9c33333729776eece94017b5dc3fda72abf40d5b97f8217233ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
